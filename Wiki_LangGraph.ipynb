{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"   
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installs"
      ],
      "metadata": {
        "id": "tOSgW5bmD1i1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdzYxueznmXI",
        "outputId": "770ae921-e655-41d6-d1be-1af2d71d8ae5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/915.7 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:36\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "^C\n",
            "^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U torch==2.10.0 torchaudio==2.10.0\n",
        "\n",
        "!pip -q install -U langgraph langchain-core requests beautifulsoup4 lxml\n",
        "\n",
        "!pip -q install -U transformers==4.41.2 accelerate\n",
        "\n",
        "!pip -q install -U diffusers safetensors pydub soundfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "sBClIDfwD-YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, random, time\n",
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "\n",
        "import requests\n",
        "\n",
        "OUT_DIR = \"/content/wiki_case_story_output\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Reduce HF “helpful” prompts; still no token required for public models\n",
        "os.environ[\"HF_HUB_DISABLE_IMPLICIT_TOKEN\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "SESSION = requests.Session()\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (t or \"\")).strip()\n",
        "\n",
        "def safe_filename(name: str) -> str:\n",
        "    name = re.sub(r\"[^a-zA-Z0-9_\\- ]+\", \"\", (name or \"\")).strip().replace(\" \", \"_\")\n",
        "    return name[:80] if name else \"case\"\n",
        "\n",
        "def extract_json(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the FIRST valid JSON object found in a model output.\n",
        "    This avoids 'Extra data' when the model prints multiple objects or commentary.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        raise ValueError(\"Empty text\")\n",
        "\n",
        "    # Fast path: try direct load\n",
        "    try:\n",
        "        json.loads(text)\n",
        "        return text\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Scan for balanced JSON objects\n",
        "    start = text.find(\"{\")\n",
        "    if start == -1:\n",
        "        raise ValueError(\"No JSON object start found\")\n",
        "\n",
        "    depth = 0\n",
        "    in_str = False\n",
        "    escape = False\n",
        "    for i in range(start, len(text)):\n",
        "        ch = text[i]\n",
        "        if in_str:\n",
        "            if escape:\n",
        "                escape = False\n",
        "            elif ch == \"\\\\\":\n",
        "                escape = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == \"{\":\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    candidate = text[start:i+1]\n",
        "                    # Validate it is JSON\n",
        "                    json.loads(candidate)\n",
        "                    return candidate\n",
        "\n",
        "    raise ValueError(\"No complete JSON object found\")\n",
        "\n",
        "\n",
        "def word_count(s: str) -> int:\n",
        "    return len((s or \"\").split())\n"
      ],
      "metadata": {
        "id": "Pj5jhSg9nprH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "y5t8X8L5EDPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# If VRAM issues: \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "LLM_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "\n",
        "def llm(prompt: str, max_new_tokens: int = 800, temperature: float = 0.6) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Local LLM ready:\", LLM_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-VpGw7wnptd",
        "outputId": "e3469862-978d-45d3-fa3a-cb4d74163520",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Local LLM ready: Qwen/Qwen2.5-1.5B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image + TTS"
      ],
      "metadata": {
        "id": "hUAajKvCEGd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Image model (fast). If this ever fails due to bandwidth, try: \"stabilityai/sdxl-turbo\"\n",
        "IMG_MODEL = \"stabilityai/sd-turbo\"\n",
        "\n",
        "img_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    IMG_MODEL,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False,\n",
        ").to(device)\n",
        "\n",
        "def generate_image(prompt: str, out_path: str):\n",
        "    im = img_pipe(prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "    im.save(out_path)\n",
        "    return out_path\n",
        "\n",
        "tts = pipeline(\n",
        "    \"text-to-speech\",\n",
        "    model=\"facebook/mms-tts-eng\",\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "def tts_to_wav(text: str, wav_path: str):\n",
        "    out = tts(text)\n",
        "    sf.write(wav_path, out[\"audio\"], out[\"sampling_rate\"])\n",
        "    return wav_path\n",
        "\n",
        "def concat_wavs(wav_paths: List[str], out_path: str):\n",
        "    combined = AudioSegment.silent(duration=200)\n",
        "    for wp in wav_paths:\n",
        "        combined += AudioSegment.from_wav(wp) + AudioSegment.silent(duration=200)\n",
        "    combined.export(out_path, format=\"wav\")\n",
        "    return out_path\n",
        "\n",
        "print(\"Image + TTS ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "0939fa77089d454a9f991d14ab242478",
            "ba57b0761634476d8321ae800c79937c",
            "2d444009c58b48618ecfdcb071cd4297",
            "236a565c6df842c69648dc088e2103ea",
            "7aef20423dbe4d068b8b3059525e81ba",
            "b78d99d695264c869e38efe0c6899501",
            "17a7fbdcc5834a0ca37d0e69ae7d7b32",
            "2efabbfa784343a4bc3b8236c0a58045",
            "e97f8d5816d6427c97f15e3e2b3158b5",
            "d49941fedd774c2cad4eef85500267b0",
            "7f51fef45de04f4891b79365460d2185"
          ]
        },
        "id": "RifRzHhPnpvz",
        "outputId": "77b1f939-41d2-417d-8e49-eab41c336184",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0939fa77089d454a9f991d14ab242478"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/mms-tts-eng were not used when initializing VitsModel: ['flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v']\n",
            "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-eng and are newly initialized: ['flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Image + TTS ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WIKI API Tool"
      ],
      "metadata": {
        "id": "P8UBFboHEKkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "CASE_QUERY_SEEDS = [\n",
        "    \"notorious criminal case United States\",\n",
        "    \"famous criminal case United States\",\n",
        "    \"high-profile kidnapping case\",\n",
        "    \"bank robbery manhunt case\",\n",
        "    \"unsolved mystery case United States\",\n",
        "    \"organized crime case United States\",\n",
        "    \"serial killer investigation case\",\n",
        "    \"famous trial case United States\",\n",
        "]\n",
        "\n",
        "BAD_TITLE_PREFIXES = (\"List of\", \"Category:\", \"Template:\", \"Help:\", \"Portal:\")\n",
        "BAD_TITLE_CONTAINS = (\"(disambiguation)\",)\n",
        "\n",
        "def wiki_search(query: str, limit: int = 10) -> List[Dict[str, Any]]:\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"srlimit\": str(limit),\n",
        "        \"format\": \"json\",\n",
        "        \"utf8\": \"1\",\n",
        "    }\n",
        "    r = SESSION.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    results = data.get(\"query\", {}).get(\"search\", [])\n",
        "    out = []\n",
        "    for item in results:\n",
        "        title = item.get(\"title\", \"\")\n",
        "        if not title or title.startswith(BAD_TITLE_PREFIXES) or any(x in title for x in BAD_TITLE_CONTAINS):\n",
        "            continue\n",
        "        out.append({\n",
        "            \"title\": title,\n",
        "            \"pageid\": item.get(\"pageid\"),\n",
        "            \"snippet\": clean_text(re.sub(r\"<.*?>\", \"\", item.get(\"snippet\", \"\"))),  # strip HTML tags from snippet\n",
        "            \"timestamp\": item.get(\"timestamp\"),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def wiki_extract(pageid: int) -> Dict[str, Any]:\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"pageids\": str(pageid),\n",
        "        \"prop\": \"extracts|info\",\n",
        "        \"explaintext\": \"1\",\n",
        "        \"exsectionformat\": \"plain\",\n",
        "        \"inprop\": \"url\",\n",
        "        \"format\": \"json\",\n",
        "        \"utf8\": \"1\",\n",
        "    }\n",
        "    r = SESSION.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    pages = r.json().get(\"query\", {}).get(\"pages\", {})\n",
        "    page = pages.get(str(pageid), {})\n",
        "    return {\n",
        "        \"title\": page.get(\"title\", \"\"),\n",
        "        \"fullurl\": page.get(\"fullurl\", \"\"),\n",
        "        \"extract\": page.get(\"extract\", \"\") or \"\",\n",
        "    }\n",
        "\n",
        "def trim_wiki_text(text: str, max_chars: int = 9000) -> str:\n",
        "    # chop off references-ish sections to keep prompt clean\n",
        "    cut_markers = [\"References\", \"External links\", \"See also\", \"Further reading\", \"Bibliography\", \"Notes\"]\n",
        "    t = text\n",
        "    for m in cut_markers:\n",
        "        idx = t.find(\"\\n\" + m + \"\\n\")\n",
        "        if idx != -1:\n",
        "            t = t[:idx]\n",
        "    t = clean_text(t)\n",
        "    return t[:max_chars]\n"
      ],
      "metadata": {
        "id": "WHv5FU5SnpyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangGraph Nodes"
      ],
      "metadata": {
        "id": "aYQMtj-BENkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "class StoryState(TypedDict, total=False):\n",
        "    genre: str\n",
        "    query: str\n",
        "    candidates: List[Dict[str, Any]]\n",
        "    chosen: Dict[str, Any]\n",
        "    source_title: str\n",
        "    source_url: str\n",
        "    source_text: str\n",
        "    story_text: str\n",
        "    sections_raw: str\n",
        "    sections: List[Dict[str, str]]\n",
        "    attempts: int\n",
        "    json_error: str\n",
        "    out_dir: str\n",
        "\n",
        "def node_make_query(state: StoryState) -> StoryState:\n",
        "    # Make it “agentic”: let LLM pick a strong query in the \"notorious cases\" space\n",
        "    seed = random.choice(CASE_QUERY_SEEDS)\n",
        "    prompt = f\"\"\"\n",
        "Generate ONE Wikipedia search query to find a compelling \"notorious criminal case\" or \"famous case\" article.\n",
        "Goal: documentary-style narrative potential (investigation, mystery, twist, trial, manhunt).\n",
        "Return ONLY JSON: {{\"query\":\"...\"}}\n",
        "\n",
        "Seed idea: {seed}\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=120, temperature=0.3)\n",
        "    try:\n",
        "        obj = json.loads(extract_json(out))\n",
        "        query = clean_text(obj.get(\"query\", \"\")) or seed\n",
        "    except Exception:\n",
        "        query = seed\n",
        "    return {**state, \"query\": query}\n",
        "\n",
        "def node_search_wiki(state: StoryState) -> StoryState:\n",
        "    query = state[\"query\"]\n",
        "    cands = wiki_search(query, limit=12)\n",
        "\n",
        "    # Lightweight quality filter: prefer likely “case-like” pages\n",
        "    keywords = [\"case\", \"murder\", \"kidnapping\", \"trial\", \"investigation\", \"robbery\", \"manhunt\", \"crime\", \"shooting\"]\n",
        "    filtered = []\n",
        "    for c in cands:\n",
        "        blob = (c[\"title\"] + \" \" + c.get(\"snippet\", \"\")).lower()\n",
        "        if any(k in blob for k in keywords):\n",
        "            filtered.append(c)\n",
        "    if len(filtered) >= 5:\n",
        "        cands = filtered\n",
        "\n",
        "    if not cands:\n",
        "        # fallback: broaden query a bit\n",
        "        cands = wiki_search(\"notorious criminal case\", limit=12)\n",
        "    return {**state, \"candidates\": cands[:10]}\n",
        "\n",
        "def node_choose_case(state: StoryState) -> StoryState:\n",
        "    cands = state[\"candidates\"][:8]\n",
        "    prompt = f\"\"\"\n",
        "Pick ONE Wikipedia article that will make the best true-crime / notorious-case narrative.\n",
        "Prefer: clear timeline, investigation/manhunt, suspense, consequences, or unresolved mystery.\n",
        "Avoid: purely biographical pages unless the case itself is central.\n",
        "\n",
        "Return ONLY JSON: {{\"title\":\"...\",\"pageid\":123,\"reason\":\"...\"}}\n",
        "\n",
        "Candidates:\n",
        "{json.dumps(cands, indent=2)}\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=240, temperature=0.2)\n",
        "\n",
        "    try:\n",
        "        obj = json.loads(extract_json(out))\n",
        "    except Exception:\n",
        "        # quick repair pass\n",
        "        repair = llm(\n",
        "            \"Convert the following into ONLY valid JSON with keys \"\n",
        "            '\"title\", \"pageid\", \"reason\". No extra text.\\n\\n'\n",
        "            f\"{out}\",\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        obj = json.loads(extract_json(repair))\n",
        "\n",
        "    picked = None\n",
        "    for c in cands:\n",
        "        if c.get(\"pageid\") == obj.get(\"pageid\") or c.get(\"title\") == obj.get(\"title\"):\n",
        "            picked = c\n",
        "            break\n",
        "    if not picked:\n",
        "        picked = cands[0]\n",
        "    return {**state, \"chosen\": picked}\n",
        "\n",
        "\n",
        "def node_fetch_wiki(state: StoryState) -> StoryState:\n",
        "    pageid = int(state[\"chosen\"][\"pageid\"])\n",
        "    data = wiki_extract(pageid)\n",
        "    text = trim_wiki_text(data[\"extract\"], max_chars=9000)\n",
        "    if word_count(text) < 250:\n",
        "        raise RuntimeError(\"Wikipedia extract is too short to build a story. Try rerun for a different case.\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"source_title\": data[\"title\"],\n",
        "        \"source_url\": data[\"fullurl\"],\n",
        "        \"source_text\": text,\n",
        "    }\n",
        "\n",
        "def node_write_story(state: StoryState) -> StoryState:\n",
        "    prompt = f\"\"\"\n",
        "Write a compelling NON-FICTION documentary-style narrative from the SOURCE TEXT.\n",
        "Rules:\n",
        "- Use ONLY facts from SOURCE TEXT (no invented events, names, or details).\n",
        "- High-school readable.\n",
        "- 900–1300 words.\n",
        "- No bullet points.\n",
        "- Keep a clear timeline with tension.\n",
        "\n",
        "TITLE: {state[\"source_title\"]}\n",
        "SOURCE URL: {state[\"source_url\"]}\n",
        "\n",
        "SOURCE TEXT:\n",
        "{state[\"source_text\"]}\n",
        "\n",
        "Return ONLY the story text.\n",
        "\"\"\"\n",
        "    story = llm(prompt, max_new_tokens=1600, temperature=0.7)\n",
        "    return {**state, \"story_text\": story}\n",
        "\n",
        "def node_split_sections(state: StoryState) -> StoryState:\n",
        "    prompt = f\"\"\"\n",
        "Split the story into exactly 6 sections.\n",
        "Return ONLY valid JSON (no markdown) in this schema:\n",
        "\n",
        "{{\n",
        "  \"sections\": [\n",
        "    {{\n",
        "      \"section_title\": \"...\",\n",
        "      \"section_text\": \"...\",\n",
        "      \"image_prompt\": \"...\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Constraints:\n",
        "- section_text must be 120–220 words each\n",
        "- image_prompt should be cinematic documentary still, descriptive, NO text in image\n",
        "- avoid graphic gore; keep it safe and documentary-like\n",
        "\n",
        "STORY:\n",
        "{state[\"story_text\"]}\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=1100, temperature=0.3)\n",
        "    return {**state, \"sections_raw\": out, \"json_error\": \"\", \"attempts\": state.get(\"attempts\", 0)}\n",
        "\n",
        "def validate_sections(sections: Any) -> Optional[str]:\n",
        "    if not isinstance(sections, list) or len(sections) != 6:\n",
        "        return \"sections must be a list of length 6\"\n",
        "    for i, s in enumerate(sections):\n",
        "        if not isinstance(s, dict):\n",
        "            return f\"section {i} must be an object\"\n",
        "        for k in [\"section_title\", \"section_text\", \"image_prompt\"]:\n",
        "            if k not in s or not isinstance(s[k], str) or not s[k].strip():\n",
        "                return f\"section {i} missing/invalid {k}\"\n",
        "        wc = len(s[\"section_text\"].split())\n",
        "        if wc < 120 or wc > 220:\n",
        "            return f\"section {i} section_text word count {wc} not in 120–220\"\n",
        "    return None\n",
        "\n",
        "def node_validate(state: StoryState) -> StoryState:\n",
        "    raw = state[\"sections_raw\"]\n",
        "    try:\n",
        "        obj = json.loads(extract_json(raw))\n",
        "        err = validate_sections(obj.get(\"sections\"))\n",
        "        if err:\n",
        "            return {**state, \"json_error\": err}\n",
        "        return {**state, \"sections\": obj[\"sections\"], \"json_error\": \"\"}\n",
        "    except Exception as e:\n",
        "        return {**state, \"json_error\": f\"JSON parse error: {e}\"}\n",
        "\n",
        "def node_fix_json(state: StoryState) -> StoryState:\n",
        "    attempts = state.get(\"attempts\", 0) + 1\n",
        "    prompt = f\"\"\"\n",
        "Fix the output into ONLY valid JSON matching this schema exactly:\n",
        "\n",
        "{{\n",
        "  \"sections\": [\n",
        "    {{\n",
        "      \"section_title\": \"...\",\n",
        "      \"section_text\": \"...\",\n",
        "      \"image_prompt\": \"...\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "There must be exactly 6 sections.\n",
        "Each section_text must be 120–220 words.\n",
        "\n",
        "Problem:\n",
        "{state.get(\"json_error\",\"\")}\n",
        "\n",
        "Bad output:\n",
        "{state[\"sections_raw\"]}\n",
        "\n",
        "Return ONLY corrected JSON.\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=1000, temperature=0.2)\n",
        "    return {**state, \"sections_raw\": out, \"attempts\": attempts}\n",
        "\n",
        "def route_after_validate(state: StoryState) -> str:\n",
        "    if not state.get(\"json_error\"):\n",
        "        return \"assets\"\n",
        "    if state.get(\"attempts\", 0) < 2:\n",
        "        return \"fix\"\n",
        "    return \"give_up\"\n",
        "\n",
        "def node_generate_assets(state: StoryState) -> StoryState:\n",
        "    slug = safe_filename(state[\"source_title\"])\n",
        "    out_dir = os.path.join(OUT_DIR, slug)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    wavs = []\n",
        "    for i, sec in enumerate(state[\"sections\"], start=1):\n",
        "        img_path = os.path.join(out_dir, f\"section_{i:02d}.png\")\n",
        "        wav_path = os.path.join(out_dir, f\"section_{i:02d}.wav\")\n",
        "\n",
        "        generate_image(sec[\"image_prompt\"], img_path)\n",
        "        tts_to_wav(sec[\"section_text\"], wav_path)\n",
        "        wavs.append(wav_path)\n",
        "\n",
        "    full_audio = os.path.join(out_dir, \"full_narration.wav\")\n",
        "    concat_wavs(wavs, full_audio)\n",
        "\n",
        "    payload = {\n",
        "        \"query\": state[\"query\"],\n",
        "        \"source_title\": state[\"source_title\"],\n",
        "        \"source_url\": state[\"source_url\"],\n",
        "        \"source_text\": state[\"source_text\"],\n",
        "        \"story_text\": state[\"story_text\"],\n",
        "        \"sections\": state[\"sections\"],\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"story_data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    html_path = os.path.join(out_dir, \"storybook.html\")\n",
        "    parts = [\n",
        "        f\"<h1>{payload['source_title']}</h1>\",\n",
        "        f\"<p><b>Wikipedia source:</b> <a href='{payload['source_url']}' target='_blank'>{payload['source_url']}</a></p>\",\n",
        "        f\"<p><b>Search query:</b> {payload['query']}</p>\",\n",
        "        \"<hr>\",\n",
        "    ]\n",
        "    for i, sec in enumerate(payload[\"sections\"], start=1):\n",
        "        parts += [\n",
        "            f\"<h2>{i}. {sec['section_title']}</h2>\",\n",
        "            f\"<img src='section_{i:02d}.png' style='max-width:900px;width:100%;border-radius:12px;'>\",\n",
        "            f\"<p style='font-size:18px;line-height:1.5'>{sec['section_text']}</p>\",\n",
        "            f\"<p><i>Prompt:</i> {sec['image_prompt']}</p>\",\n",
        "            \"<hr>\",\n",
        "        ]\n",
        "    parts += [\n",
        "        \"<h3>Full narration audio</h3>\",\n",
        "        \"<audio controls src='full_narration.wav'></audio>\",\n",
        "    ]\n",
        "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(parts))\n",
        "\n",
        "    return {**state, \"out_dir\": out_dir}\n",
        "\n",
        "def node_give_up(state: StoryState) -> StoryState:\n",
        "    raise ValueError(f\"Failed to produce valid sections JSON after retries. Last error: {state.get('json_error')}\")\n"
      ],
      "metadata": {
        "id": "qHSg9y7Fnp0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build LangGraph"
      ],
      "metadata": {
        "id": "1Lvr8bhrESRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = StateGraph(StoryState)\n",
        "\n",
        "g.add_node(\"make_query\", node_make_query)\n",
        "g.add_node(\"search\", node_search_wiki)\n",
        "g.add_node(\"choose\", node_choose_case)\n",
        "g.add_node(\"fetch\", node_fetch_wiki)\n",
        "g.add_node(\"story\", node_write_story)\n",
        "g.add_node(\"split\", node_split_sections)\n",
        "g.add_node(\"validate\", node_validate)\n",
        "g.add_node(\"fix\", node_fix_json)\n",
        "g.add_node(\"assets\", node_generate_assets)\n",
        "g.add_node(\"give_up\", node_give_up)\n",
        "\n",
        "g.set_entry_point(\"make_query\")\n",
        "g.add_edge(\"make_query\", \"search\")\n",
        "g.add_edge(\"search\", \"choose\")\n",
        "g.add_edge(\"choose\", \"fetch\")\n",
        "g.add_edge(\"fetch\", \"story\")\n",
        "g.add_edge(\"story\", \"split\")\n",
        "g.add_edge(\"split\", \"validate\")\n",
        "\n",
        "g.add_conditional_edges(\n",
        "    \"validate\",\n",
        "    route_after_validate,\n",
        "    {\"assets\": \"assets\", \"fix\": \"fix\", \"give_up\": \"give_up\"},\n",
        ")\n",
        "g.add_edge(\"fix\", \"validate\")\n",
        "g.add_edge(\"assets\", END)\n",
        "\n",
        "app = g.compile()\n",
        "print(\"Graph compiled. Running...\")\n",
        "\n",
        "final_state = app.invoke({\"attempts\": 0})\n",
        "print(\"\\n Done!\")\n",
        "print(\"Query:\", final_state[\"query\"])\n",
        "print(\"Chosen:\", final_state[\"source_title\"])\n",
        "print(\"Output folder:\", final_state[\"out_dir\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U_vZ780rnp2k",
        "outputId": "5c090ba5-f01d-4d51-fc0c-4248e83d5f57",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Graph compiled. Running...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to produce valid sections JSON after retries. Last error: sections must be a list of length 6",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-709454173.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Graph compiled. Running...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"attempts\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✅ Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Query:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3072\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2647\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2382175551.py\u001b[0m in \u001b[0;36mnode_give_up\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnode_give_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStoryState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStoryState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to produce valid sections JSON after retries. Last error: {state.get('json_error')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Failed to produce valid sections JSON after retries. Last error: sections must be a list of length 6"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SoBHViiQnp5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGH6YeNjnp7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "387k0mDQnp9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3Iqwnyunp_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkZLoEBBnqCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}