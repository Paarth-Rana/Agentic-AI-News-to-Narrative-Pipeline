{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installs"
      ],
      "metadata": {
        "id": "tOSgW5bmD1i1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdzYxueznmXI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip -q install -U torch==2.10.0 torchaudio==2.10.0\n",
        "\n",
        "!pip -q install -U langgraph langchain-core requests beautifulsoup4 lxml\n",
        "\n",
        "!pip -q install -U transformers==4.41.2 accelerate\n",
        "\n",
        "!pip -q install -U diffusers safetensors pydub soundfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "sBClIDfwD-YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, random, time\n",
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "\n",
        "import requests\n",
        "\n",
        "OUT_DIR = \"/content/wiki_case_story_output\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Reduce HF “helpful” prompts; still no token required for public models\n",
        "os.environ[\"HF_HUB_DISABLE_IMPLICIT_TOKEN\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "SESSION = requests.Session()\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (t or \"\")).strip()\n",
        "\n",
        "def safe_filename(name: str) -> str:\n",
        "    name = re.sub(r\"[^a-zA-Z0-9_\\- ]+\", \"\", (name or \"\")).strip().replace(\" \", \"_\")\n",
        "    return name[:80] if name else \"case\"\n",
        "\n",
        "def extract_json(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the FIRST valid JSON object found in a model output.\n",
        "    This avoids 'Extra data' when the model prints multiple objects or commentary.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        raise ValueError(\"Empty text\")\n",
        "\n",
        "    # Fast path: try direct load\n",
        "    try:\n",
        "        json.loads(text)\n",
        "        return text\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Scan for balanced JSON objects\n",
        "    start = text.find(\"{\")\n",
        "    if start == -1:\n",
        "        raise ValueError(\"No JSON object start found\")\n",
        "\n",
        "    depth = 0\n",
        "    in_str = False\n",
        "    escape = False\n",
        "    for i in range(start, len(text)):\n",
        "        ch = text[i]\n",
        "        if in_str:\n",
        "            if escape:\n",
        "                escape = False\n",
        "            elif ch == \"\\\\\":\n",
        "                escape = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == \"{\":\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    candidate = text[start:i+1]\n",
        "                    # Validate it is JSON\n",
        "                    json.loads(candidate)\n",
        "                    return candidate\n",
        "\n",
        "    raise ValueError(\"No complete JSON object found\")\n",
        "\n",
        "\n",
        "def word_count(s: str) -> int:\n",
        "    return len((s or \"\").split())\n"
      ],
      "metadata": {
        "id": "Pj5jhSg9nprH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "y5t8X8L5EDPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# If VRAM issues: \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "LLM_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "\n",
        "def llm(prompt: str, max_new_tokens: int = 800, temperature: float = 0.6) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Local LLM ready:\", LLM_NAME)\n"
      ],
      "metadata": {
        "id": "U-VpGw7wnptd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image + TTS"
      ],
      "metadata": {
        "id": "hUAajKvCEGd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Image model (fast). If this ever fails due to bandwidth, try: \"stabilityai/sdxl-turbo\"\n",
        "IMG_MODEL = \"stabilityai/sd-turbo\"\n",
        "\n",
        "img_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    IMG_MODEL,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False,\n",
        ").to(device)\n",
        "\n",
        "def generate_image(prompt: str, out_path: str):\n",
        "    im = img_pipe(prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "    im.save(out_path)\n",
        "    return out_path\n",
        "\n",
        "tts = pipeline(\n",
        "    \"text-to-speech\",\n",
        "    model=\"facebook/mms-tts-eng\",\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "def tts_to_wav(text: str, wav_path: str):\n",
        "    out = tts(text)\n",
        "    sf.write(wav_path, out[\"audio\"], out[\"sampling_rate\"])\n",
        "    return wav_path\n",
        "\n",
        "def concat_wavs(wav_paths: List[str], out_path: str):\n",
        "    combined = AudioSegment.silent(duration=200)\n",
        "    for wp in wav_paths:\n",
        "        combined += AudioSegment.from_wav(wp) + AudioSegment.silent(duration=200)\n",
        "    combined.export(out_path, format=\"wav\")\n",
        "    return out_path\n",
        "\n",
        "print(\"Image + TTS ready.\")\n"
      ],
      "metadata": {
        "id": "RifRzHhPnpvz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WIKI API Tool"
      ],
      "metadata": {
        "id": "P8UBFboHEKkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "CASE_QUERY_SEEDS = [\n",
        "    \"notorious criminal case United States\",\n",
        "    \"famous criminal case United States\",\n",
        "    \"high-profile kidnapping case\",\n",
        "    \"bank robbery manhunt case\",\n",
        "    \"unsolved mystery case United States\",\n",
        "    \"organized crime case United States\",\n",
        "    \"serial killer investigation case\",\n",
        "    \"famous trial case United States\",\n",
        "]\n",
        "\n",
        "BAD_TITLE_PREFIXES = (\"List of\", \"Category:\", \"Template:\", \"Help:\", \"Portal:\")\n",
        "BAD_TITLE_CONTAINS = (\"(disambiguation)\",)\n",
        "\n",
        "def wiki_search(query: str, limit: int = 10) -> List[Dict[str, Any]]:\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"srlimit\": str(limit),\n",
        "        \"format\": \"json\",\n",
        "        \"utf8\": \"1\",\n",
        "    }\n",
        "    r = SESSION.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    results = data.get(\"query\", {}).get(\"search\", [])\n",
        "    out = []\n",
        "    for item in results:\n",
        "        title = item.get(\"title\", \"\")\n",
        "        if not title or title.startswith(BAD_TITLE_PREFIXES) or any(x in title for x in BAD_TITLE_CONTAINS):\n",
        "            continue\n",
        "        out.append({\n",
        "            \"title\": title,\n",
        "            \"pageid\": item.get(\"pageid\"),\n",
        "            \"snippet\": clean_text(re.sub(r\"<.*?>\", \"\", item.get(\"snippet\", \"\"))),  # strip HTML tags from snippet\n",
        "            \"timestamp\": item.get(\"timestamp\"),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def wiki_extract(pageid: int) -> Dict[str, Any]:\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"pageids\": str(pageid),\n",
        "        \"prop\": \"extracts|info\",\n",
        "        \"explaintext\": \"1\",\n",
        "        \"exsectionformat\": \"plain\",\n",
        "        \"inprop\": \"url\",\n",
        "        \"format\": \"json\",\n",
        "        \"utf8\": \"1\",\n",
        "    }\n",
        "    r = SESSION.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    pages = r.json().get(\"query\", {}).get(\"pages\", {})\n",
        "    page = pages.get(str(pageid), {})\n",
        "    return {\n",
        "        \"title\": page.get(\"title\", \"\"),\n",
        "        \"fullurl\": page.get(\"fullurl\", \"\"),\n",
        "        \"extract\": page.get(\"extract\", \"\") or \"\",\n",
        "    }\n",
        "\n",
        "def trim_wiki_text(text: str, max_chars: int = 9000) -> str:\n",
        "    # chop off references-ish sections to keep prompt clean\n",
        "    cut_markers = [\"References\", \"External links\", \"See also\", \"Further reading\", \"Bibliography\", \"Notes\"]\n",
        "    t = text\n",
        "    for m in cut_markers:\n",
        "        idx = t.find(\"\\n\" + m + \"\\n\")\n",
        "        if idx != -1:\n",
        "            t = t[:idx]\n",
        "    t = clean_text(t)\n",
        "    return t[:max_chars]\n"
      ],
      "metadata": {
        "id": "WHv5FU5SnpyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangGraph Nodes"
      ],
      "metadata": {
        "id": "aYQMtj-BENkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "class StoryState(TypedDict, total=False):\n",
        "    genre: str\n",
        "    query: str\n",
        "    candidates: List[Dict[str, Any]]\n",
        "    chosen: Dict[str, Any]\n",
        "    source_title: str\n",
        "    source_url: str\n",
        "    source_text: str\n",
        "    story_text: str\n",
        "    sections_raw: str\n",
        "    sections: List[Dict[str, str]]\n",
        "    attempts: int\n",
        "    json_error: str\n",
        "    out_dir: str\n",
        "\n",
        "def node_make_query(state: StoryState) -> StoryState:\n",
        "    # Make it “agentic”: let LLM pick a strong query in the \"notorious cases\" space\n",
        "    seed = random.choice(CASE_QUERY_SEEDS)\n",
        "    prompt = f\"\"\"\n",
        "Generate ONE Wikipedia search query to find a compelling \"notorious criminal case\" or \"famous case\" article.\n",
        "Goal: documentary-style narrative potential (investigation, mystery, twist, trial, manhunt).\n",
        "Return ONLY JSON: {{\"query\":\"...\"}}\n",
        "\n",
        "Seed idea: {seed}\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=120, temperature=0.3)\n",
        "    try:\n",
        "        obj = json.loads(extract_json(out))\n",
        "        query = clean_text(obj.get(\"query\", \"\")) or seed\n",
        "    except Exception:\n",
        "        query = seed\n",
        "    return {**state, \"query\": query}\n",
        "\n",
        "def node_search_wiki(state: StoryState) -> StoryState:\n",
        "    query = state[\"query\"]\n",
        "    cands = wiki_search(query, limit=12)\n",
        "\n",
        "    # Lightweight quality filter: prefer likely “case-like” pages\n",
        "    keywords = [\"case\", \"murder\", \"kidnapping\", \"trial\", \"investigation\", \"robbery\", \"manhunt\", \"crime\", \"shooting\"]\n",
        "    filtered = []\n",
        "    for c in cands:\n",
        "        blob = (c[\"title\"] + \" \" + c.get(\"snippet\", \"\")).lower()\n",
        "        if any(k in blob for k in keywords):\n",
        "            filtered.append(c)\n",
        "    if len(filtered) >= 5:\n",
        "        cands = filtered\n",
        "\n",
        "    if not cands:\n",
        "        # fallback: broaden query a bit\n",
        "        cands = wiki_search(\"notorious criminal case\", limit=12)\n",
        "    return {**state, \"candidates\": cands[:10]}\n",
        "\n",
        "def node_choose_case(state: StoryState) -> StoryState:\n",
        "    cands = state[\"candidates\"][:8]\n",
        "    prompt = f\"\"\"\n",
        "Pick ONE Wikipedia article that will make the best true-crime / notorious-case narrative.\n",
        "Prefer: clear timeline, investigation/manhunt, suspense, consequences, or unresolved mystery.\n",
        "Avoid: purely biographical pages unless the case itself is central.\n",
        "\n",
        "Return ONLY JSON: {{\"title\":\"...\",\"pageid\":123,\"reason\":\"...\"}}\n",
        "\n",
        "Candidates:\n",
        "{json.dumps(cands, indent=2)}\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=240, temperature=0.2)\n",
        "\n",
        "    try:\n",
        "        obj = json.loads(extract_json(out))\n",
        "    except Exception:\n",
        "        # quick repair pass\n",
        "        repair = llm(\n",
        "            \"Convert the following into ONLY valid JSON with keys \"\n",
        "            '\"title\", \"pageid\", \"reason\". No extra text.\\n\\n'\n",
        "            f\"{out}\",\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        obj = json.loads(extract_json(repair))\n",
        "\n",
        "    picked = None\n",
        "    for c in cands:\n",
        "        if c.get(\"pageid\") == obj.get(\"pageid\") or c.get(\"title\") == obj.get(\"title\"):\n",
        "            picked = c\n",
        "            break\n",
        "    if not picked:\n",
        "        picked = cands[0]\n",
        "    return {**state, \"chosen\": picked}\n",
        "\n",
        "\n",
        "def node_fetch_wiki(state: StoryState) -> StoryState:\n",
        "    pageid = int(state[\"chosen\"][\"pageid\"])\n",
        "    data = wiki_extract(pageid)\n",
        "    text = trim_wiki_text(data[\"extract\"], max_chars=9000)\n",
        "    if word_count(text) < 250:\n",
        "        raise RuntimeError(\"Wikipedia extract is too short to build a story. Try rerun for a different case.\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"source_title\": data[\"title\"],\n",
        "        \"source_url\": data[\"fullurl\"],\n",
        "        \"source_text\": text,\n",
        "    }\n",
        "\n",
        "def node_write_story(state: StoryState) -> StoryState:\n",
        "    prompt = f\"\"\"\n",
        "Write a compelling NON-FICTION documentary-style narrative from the SOURCE TEXT.\n",
        "Rules:\n",
        "- Use ONLY facts from SOURCE TEXT (no invented events, names, or details).\n",
        "- High-school readable.\n",
        "- 900–1300 words.\n",
        "- No bullet points.\n",
        "- Keep a clear timeline with tension.\n",
        "\n",
        "TITLE: {state[\"source_title\"]}\n",
        "SOURCE URL: {state[\"source_url\"]}\n",
        "\n",
        "SOURCE TEXT:\n",
        "{state[\"source_text\"]}\n",
        "\n",
        "Return ONLY the story text.\n",
        "\"\"\"\n",
        "    story = llm(prompt, max_new_tokens=1600, temperature=0.7)\n",
        "    return {**state, \"story_text\": story}\n",
        "\n",
        "def node_split_sections(state: StoryState) -> StoryState:\n",
        "    prompt = f\"\"\"\n",
        "Split the story into exactly 6 sections.\n",
        "Return ONLY valid JSON (no markdown) in this schema:\n",
        "\n",
        "{{\n",
        "  \"sections\": [\n",
        "    {{\n",
        "      \"section_title\": \"...\",\n",
        "      \"section_text\": \"...\",\n",
        "      \"image_prompt\": \"...\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Constraints:\n",
        "- section_text must be 120–220 words each\n",
        "- image_prompt should be cinematic documentary still, descriptive, NO text in image\n",
        "- avoid graphic gore; keep it safe and documentary-like\n",
        "\n",
        "STORY:\n",
        "{state[\"story_text\"]}\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=1100, temperature=0.3)\n",
        "    return {**state, \"sections_raw\": out, \"json_error\": \"\", \"attempts\": state.get(\"attempts\", 0)}\n",
        "\n",
        "def validate_sections(sections: Any) -> Optional[str]:\n",
        "    if not isinstance(sections, list) or len(sections) != 6:\n",
        "        return \"sections must be a list of length 6\"\n",
        "    for i, s in enumerate(sections):\n",
        "        if not isinstance(s, dict):\n",
        "            return f\"section {i} must be an object\"\n",
        "        for k in [\"section_title\", \"section_text\", \"image_prompt\"]:\n",
        "            if k not in s or not isinstance(s[k], str) or not s[k].strip():\n",
        "                return f\"section {i} missing/invalid {k}\"\n",
        "        wc = len(s[\"section_text\"].split())\n",
        "        if wc < 120 or wc > 220:\n",
        "            return f\"section {i} section_text word count {wc} not in 120–220\"\n",
        "    return None\n",
        "\n",
        "def node_validate(state: StoryState) -> StoryState:\n",
        "    raw = state[\"sections_raw\"]\n",
        "    try:\n",
        "        obj = json.loads(extract_json(raw))\n",
        "        err = validate_sections(obj.get(\"sections\"))\n",
        "        if err:\n",
        "            return {**state, \"json_error\": err}\n",
        "        return {**state, \"sections\": obj[\"sections\"], \"json_error\": \"\"}\n",
        "    except Exception as e:\n",
        "        return {**state, \"json_error\": f\"JSON parse error: {e}\"}\n",
        "\n",
        "def node_fix_json(state: StoryState) -> StoryState:\n",
        "    attempts = state.get(\"attempts\", 0) + 1\n",
        "    prompt = f\"\"\"\n",
        "Fix the output into ONLY valid JSON matching this schema exactly:\n",
        "\n",
        "{{\n",
        "  \"sections\": [\n",
        "    {{\n",
        "      \"section_title\": \"...\",\n",
        "      \"section_text\": \"...\",\n",
        "      \"image_prompt\": \"...\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "There must be exactly 6 sections.\n",
        "Each section_text must be 120–220 words.\n",
        "\n",
        "Problem:\n",
        "{state.get(\"json_error\",\"\")}\n",
        "\n",
        "Bad output:\n",
        "{state[\"sections_raw\"]}\n",
        "\n",
        "Return ONLY corrected JSON.\n",
        "\"\"\"\n",
        "    out = llm(prompt, max_new_tokens=1000, temperature=0.2)\n",
        "    return {**state, \"sections_raw\": out, \"attempts\": attempts}\n",
        "\n",
        "def route_after_validate(state: StoryState) -> str:\n",
        "    if not state.get(\"json_error\"):\n",
        "        return \"assets\"\n",
        "    if state.get(\"attempts\", 0) < 2:\n",
        "        return \"fix\"\n",
        "    return \"give_up\"\n",
        "\n",
        "def node_generate_assets(state: StoryState) -> StoryState:\n",
        "    slug = safe_filename(state[\"source_title\"])\n",
        "    out_dir = os.path.join(OUT_DIR, slug)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    wavs = []\n",
        "    for i, sec in enumerate(state[\"sections\"], start=1):\n",
        "        img_path = os.path.join(out_dir, f\"section_{i:02d}.png\")\n",
        "        wav_path = os.path.join(out_dir, f\"section_{i:02d}.wav\")\n",
        "\n",
        "        generate_image(sec[\"image_prompt\"], img_path)\n",
        "        tts_to_wav(sec[\"section_text\"], wav_path)\n",
        "        wavs.append(wav_path)\n",
        "\n",
        "    full_audio = os.path.join(out_dir, \"full_narration.wav\")\n",
        "    concat_wavs(wavs, full_audio)\n",
        "\n",
        "    payload = {\n",
        "        \"query\": state[\"query\"],\n",
        "        \"source_title\": state[\"source_title\"],\n",
        "        \"source_url\": state[\"source_url\"],\n",
        "        \"source_text\": state[\"source_text\"],\n",
        "        \"story_text\": state[\"story_text\"],\n",
        "        \"sections\": state[\"sections\"],\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"story_data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    html_path = os.path.join(out_dir, \"storybook.html\")\n",
        "    parts = [\n",
        "        f\"<h1>{payload['source_title']}</h1>\",\n",
        "        f\"<p><b>Wikipedia source:</b> <a href='{payload['source_url']}' target='_blank'>{payload['source_url']}</a></p>\",\n",
        "        f\"<p><b>Search query:</b> {payload['query']}</p>\",\n",
        "        \"<hr>\",\n",
        "    ]\n",
        "    for i, sec in enumerate(payload[\"sections\"], start=1):\n",
        "        parts += [\n",
        "            f\"<h2>{i}. {sec['section_title']}</h2>\",\n",
        "            f\"<img src='section_{i:02d}.png' style='max-width:900px;width:100%;border-radius:12px;'>\",\n",
        "            f\"<p style='font-size:18px;line-height:1.5'>{sec['section_text']}</p>\",\n",
        "            f\"<p><i>Prompt:</i> {sec['image_prompt']}</p>\",\n",
        "            \"<hr>\",\n",
        "        ]\n",
        "    parts += [\n",
        "        \"<h3>Full narration audio</h3>\",\n",
        "        \"<audio controls src='full_narration.wav'></audio>\",\n",
        "    ]\n",
        "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(parts))\n",
        "\n",
        "    return {**state, \"out_dir\": out_dir}\n",
        "\n",
        "def node_give_up(state: StoryState) -> StoryState:\n",
        "    raise ValueError(f\"Failed to produce valid sections JSON after retries. Last error: {state.get('json_error')}\")\n"
      ],
      "metadata": {
        "id": "qHSg9y7Fnp0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build LangGraph"
      ],
      "metadata": {
        "id": "1Lvr8bhrESRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = StateGraph(StoryState)\n",
        "\n",
        "g.add_node(\"make_query\", node_make_query)\n",
        "g.add_node(\"search\", node_search_wiki)\n",
        "g.add_node(\"choose\", node_choose_case)\n",
        "g.add_node(\"fetch\", node_fetch_wiki)\n",
        "g.add_node(\"story\", node_write_story)\n",
        "g.add_node(\"split\", node_split_sections)\n",
        "g.add_node(\"validate\", node_validate)\n",
        "g.add_node(\"fix\", node_fix_json)\n",
        "g.add_node(\"assets\", node_generate_assets)\n",
        "g.add_node(\"give_up\", node_give_up)\n",
        "\n",
        "g.set_entry_point(\"make_query\")\n",
        "g.add_edge(\"make_query\", \"search\")\n",
        "g.add_edge(\"search\", \"choose\")\n",
        "g.add_edge(\"choose\", \"fetch\")\n",
        "g.add_edge(\"fetch\", \"story\")\n",
        "g.add_edge(\"story\", \"split\")\n",
        "g.add_edge(\"split\", \"validate\")\n",
        "\n",
        "g.add_conditional_edges(\n",
        "    \"validate\",\n",
        "    route_after_validate,\n",
        "    {\"assets\": \"assets\", \"fix\": \"fix\", \"give_up\": \"give_up\"},\n",
        ")\n",
        "g.add_edge(\"fix\", \"validate\")\n",
        "g.add_edge(\"assets\", END)\n",
        "\n",
        "app = g.compile()\n",
        "print(\"Graph compiled. Running...\")\n",
        "\n",
        "final_state = app.invoke({\"attempts\": 0})\n",
        "print(\"\\n Done!\")\n",
        "print(\"Query:\", final_state[\"query\"])\n",
        "print(\"Chosen:\", final_state[\"source_title\"])\n",
        "print(\"Output folder:\", final_state[\"out_dir\"])\n"
      ],
      "metadata": {
        "id": "U_vZ780rnp2k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SoBHViiQnp5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGH6YeNjnp7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "387k0mDQnp9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3Iqwnyunp_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkZLoEBBnqCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}